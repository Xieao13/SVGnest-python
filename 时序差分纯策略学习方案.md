# 基于时序差分的纯策略学习方案

## 1. 整体架构设计

### 1.1 策略网络设计
- **基础架构**：使用现有的`PlacementOnlyNetwork`作为基础
- **输入编码**：仅使用零件特征(宽高)，移除容器信息和位置编码
- **输出**：每个时间步输出所有零件的选择概率分布
- **自回归预测**：使用真正的autoregressive机制，每步基于已选择的零件序列预测下一个零件

### 1.2 纯策略学习算法
- **算法选择**：基于时序差分的策略梯度算法
- **无基线**：不使用价值网络，直接使用即时奖励作为奖励信号
- **策略更新**：每一步都计算策略梯度并更新参数

## 2. 时序差分 vs 蒙特卡洛

### 2.1 核心区别
- **蒙特卡洛**：只在episode结束时给出总reward，然后反向传播
- **时序差分**：每一步都计算即时reward，实时更新策略

### 2.2 时序差分优势
- **即时反馈**：每步都能获得学习信号
- **快速收敛**：不需要等待完整episode
- **在线学习**：可以实时适应环境变化
- **持续探索**：每步的奖励信号都能指导探索

## 3. 奖励函数设计

### 3.1 总体架构
奖励函数采用加权线性组合形式：
```
R(t) = w₁ × R_compactness(t) + w₂ × R_fitting(t) + w₃ × R_utilization(t)
```
其中权重系数满足归一化约束：w₁ + w₂ + w₃ = 1

### 3.2 紧凑度奖励 R_compactness(t)
- **定义**：已排样零件总面积与包围盒面积的比值
- **计算公式**：
  ```
  R_compactness(t) = (∑ᵢ₌₁ⁿ Aᵢ) / BBox_area(t)
  ```
- **包围盒面积**：
  ```
  BBox_area(t) = (x_max - x_min) × (y_max - y_min)
  ```
- **实现方式**：通过PlacementWorker获取已放置零件的边界框信息
- **即时计算**：每步计算当前所有已放置零件的紧凑度

### 3.3 贴合度奖励 R_fitting(t)
- **定义**：新排样零件与板材边界及已排样零件的贴合程度
- **计算公式**：
  ```
  R_fitting(t) = min(1, L_contact / (0.25 × L_perimeter))
  ```
- **贴合边长度**：新零件与板材边界、其他零件边界的接触长度总和
- **周长**：新排样零件的周长
- **膨胀处理**：考虑切割刀具物理约束，对零件轮廓进行最小间距膨胀
- **即时计算**：只计算新放置零件的贴合度贡献

### 3.4 材料利用率奖励 R_utilization(t)
- **定义**：基于废料面积变化的动态奖励
- **计算公式**：
  ```
  R_utilization(t) = {
    +1.0,  if ΔWaste ≤ 0 (废料面积减少或不变)
    -ΔWaste/Waste_prev,  if ΔWaste > 0 (废料面积增加)
  }
  ```
- **废料面积变化**：ΔWaste = Waste_current - Waste_previous
- **分段函数**：正向激励减少废料，负向反馈增加废料
- **即时计算**：计算当前步相对于上一步的废料面积变化

## 4. 时序差分策略梯度算法

### 4.1 核心思想
- **即时奖励**：每放置一个零件后立即计算reward
- **在线更新**：每一步都更新策略网络参数
- **无需等待**：不需要等待episode结束

### 4.2 算法流程
```
for each step t in episode:
    1. 根据当前策略选择动作 a_t
    2. 执行动作，获得即时奖励 r_t
    3. 计算策略梯度：∇log π(a_t|s_t) × r_t
    4. 立即更新网络参数：θ ← θ + α × ∇log π(a_t|s_t) × r_t
```

### 4.3 即时梯度计算
```python
# 每一步都计算并应用梯度
for t in range(sequence_length):
    # 1. 前向传播得到动作概率
    action_probs = policy_network(state_t)
    
    # 2. 采样动作
    action_t = sample_from_distribution(action_probs)
    
    # 3. 执行动作，获得即时奖励
    reward_t = calculate_immediate_reward(action_t, current_layout)
    
    # 4. 计算策略梯度
    log_prob = log(action_probs[action_t])
    policy_gradient = grad(log_prob) * reward_t
    
    # 5. 立即更新参数
    optimizer.step(policy_gradient)
    
    # 6. 更新状态
    state_t = update_state(state_t, action_t)
```

## 5. 奖励信号设计

### 5.1 奖励类型
为了使时序差分更有效，可以设计不同类型的即时奖励：

**绝对奖励**：当前状态的绝对质量
```python
R_absolute(t) = current_compactness + current_fitting + current_utilization
```

**增量奖励**：相对于上一步的改进
```python
R_incremental(t) = R_absolute(t) - R_absolute(t-1)
```

**混合奖励**：结合绝对和增量
```python
R_mixed(t) = α × R_absolute(t) + (1-α) × R_incremental(t)
```

### 5.2 即时奖励计算的优化
```python
def calculate_step_reward(new_part_idx, current_layout):
    # 1. 通过PlacementWorker放置新零件
    new_layout = placement_worker.place_part(current_layout, new_part_idx)
    
    # 2. 计算三维奖励
    compactness = calculate_compactness(new_layout)
    fitting = calculate_fitting(new_part_idx, new_layout)
    utilization = calculate_utilization_change(current_layout, new_layout)
    
    # 3. 加权合成
    reward = w1 * compactness + w2 * fitting + w3 * utilization
    
    return reward
```

## 6. 训练流程

### 6.1 在线训练循环
```python
for episode in training_episodes:
    state = reset_environment()
    
    for step in range(max_steps):
        # 1. 策略前向传播
        action_logits = policy_network(state)
        action_probs = softmax(action_logits)
        
        # 2. 采样动作
        action = sample_action(action_probs)
        
        # 3. 执行动作，获得即时奖励
        reward = calculate_step_reward(action, current_layout)
        
        # 4. 计算策略梯度并更新
        loss = -log(action_probs[action]) * reward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # 5. 更新状态
        state = update_state(state, action)
        
        # 6. 记录统计信息
        log_step_metrics(step, reward, action_probs)
```

### 6.2 Episode采样
1. 从训练集随机采样bin和parts组合
2. 重置环境状态
3. 使用当前策略网络进行autoregressive采样
4. 每步立即计算奖励并更新策略

### 6.3 策略更新
1. **梯度计算**：∇θ J(θ) = ∇θ log π(a|s) × R
2. **即时更新**：每步都应用梯度更新
3. **梯度裁剪**：防止梯度爆炸
4. **学习率调度**：可以使用步级别的学习率调度

## 7. 技术实现细节

### 7.1 状态表示
- **零件特征矩阵**：[batch_size, max_parts, 2] (宽度, 高度)
- **已选择mask**：[batch_size, max_parts] (0/1标记)
- **有效长度**：每个样本的实际零件数量

### 7.2 动作采样
- **概率采样**：从策略网络输出的概率分布中采样
- **温度参数**：控制探索程度，训练初期高温度，后期低温度
- **Mask约束**：确保不重复选择已放置零件

### 7.3 状态更新机制
- **增量状态**：每步只更新必要的状态信息
- **历史记录**：维护已放置零件的历史信息
- **mask更新**：实时更新可选择零件的mask

### 7.4 奖励计算优化
- **缓存机制**：缓存中间计算结果，避免重复计算
- **增量计算**：只计算新增零件对奖励的影响
- **并行计算**：批次内样本的奖励计算可并行进行

### 7.5 梯度更新策略
- **学习率调度**：可以使用步级别的学习率调度
- **梯度累积**：可以累积几步的梯度再更新
- **正则化**：添加熵正则化鼓励探索

## 8. 训练策略

### 8.1 探索策略
- **温度退火**：逐步降低采样温度，从探索转向利用
- **熵正则化**：在损失函数中加入熵项，鼓励探索
- **权重调整**：训练过程中动态调整三个奖励分量的权重

### 8.2 稳定性措施
- **奖励标准化**：使用移动平均对奖励进行标准化
- **梯度裁剪**：限制梯度范数，防止训练不稳定
- **学习率预热**：训练初期使用较小学习率

### 8.3 评估指标
- **主要指标**：平均材料利用率、紧凑度、贴合度
- **收敛指标**：策略熵、奖励方差、梯度范数
- **效率指标**：训练时间、样本效率

## 9. 与现有代码整合

### 9.1 网络修改
- 移除`PlacementOnlyNetwork`中的bin_encoder部分
- 简化输入处理，只保留零件特征编码
- 保持autoregressive的pointer network结构

### 9.2 环境适配
- 修改`BinPackingEnvironment`的奖励计算逻辑
- 集成三维奖励函数的计算
- 优化PlacementWorker的调用效率

### 9.3 训练循环
- 实现时序差分策略梯度算法的训练循环
- 集成即时奖励计算和策略更新
- 添加训练监控和日志记录

## 10. 预期效果

### 10.1 学习效率
- **即时反馈**：每步都能获得学习信号，加快收敛
- **快速收敛**：不需要等待完整episode
- **在线学习**：可以实时适应环境变化

### 10.2 探索能力
- **持续探索**：每步的奖励信号都能指导探索
- **局部优化**：能够学习到局部最优的放置策略
- **实时调整**：可以根据即时反馈调整策略

### 10.3 性能优势
- **样本效率**：相比需要完整episode的方法，样本利用率更高
- **稳定性**：即时奖励信号提供更稳定的学习信号
- **可解释性**：每步的奖励都有明确的物理意义

## 11. 实施步骤

### 11.1 第一阶段：基础框架
1. 修改`PlacementOnlyNetwork`，移除bin编码
2. 实现三维奖励函数的计算
3. 集成PlacementWorker的即时调用

### 11.2 第二阶段：算法实现
1. 实现时序差分策略梯度算法
2. 实现即时奖励计算和策略更新
3. 添加训练监控和日志记录

### 11.3 第三阶段：优化调试
1. 调试奖励函数的权重系数
2. 优化训练超参数
3. 性能测试和对比分析

这种基于时序差分的纯策略学习方法更适合排样问题，因为每个零件的放置都有明确的即时效果，可以立即通过PlacementWorker评估并给出反馈，从而实现更高效的在线学习。 