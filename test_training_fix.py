#!/usr/bin/env python3
"""
æµ‹è¯•è®­ç»ƒä¿®å¤çš„è„šæœ¬
"""

import torch
import numpy as np
from src.dataset.dataset import PlacementOnlyDataset, PlacementOnly_collate_fn
from src.model.placement_only_model import PlacementOnlyNetwork
from src.loss.placement_only_loss import calculate_placement_loss

def test_training_step():
    """æµ‹è¯•å•ä¸ªè®­ç»ƒæ­¥éª¤"""
    print("=" * 80)
    print("ğŸ” æµ‹è¯•è®­ç»ƒæ­¥éª¤")
    print("=" * 80)
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
    sample1 = {
        'bin': torch.tensor([10.0, 8.0]),
        'parts': torch.tensor([[2.0, 3.0], [1.5, 2.0]]),  # 2ä¸ªé›¶ä»¶
        'placement_order': torch.tensor([0, 1]),
        'efficiency': 0.85,
        'valid_length': 2
    }
    
    sample2 = {
        'bin': torch.tensor([10.0, 8.0]),
        'parts': torch.tensor([[1.0, 1.0], [2.0, 2.0], [1.5, 1.5]]),  # 3ä¸ªé›¶ä»¶
        'placement_order': torch.tensor([1, 0, 2]),
        'efficiency': 0.92,
        'valid_length': 3
    }
    
    batch = [sample1, sample2]
    collated_batch = PlacementOnly_collate_fn(batch)
    
    print("ğŸ“Š Batchæ•°æ®:")
    print(f"  parts shape: {collated_batch['parts'].shape}")
    print(f"  placement_orders: {collated_batch['placement_orders']}")
    print(f"  valid_lengths: {collated_batch['valid_lengths']}")
    
    # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = PlacementOnlyNetwork(input_dim=2, hidden_dim=32, seq_len=10)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
    
    model.train()
    
    # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
    parts = collated_batch['parts']
    bins = collated_batch['bins']
    placement_targets = collated_batch['placement_orders']
    valid_lengths = collated_batch['valid_lengths']
    
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒæ­¥éª¤...")
    
    # å‰å‘ä¼ æ’­
    placement_logits = model(
        parts=parts, 
        bin_info=bins,
        target_sequence=placement_targets,
        valid_lengths=valid_lengths
    )
    
    print(f"  placement_logits shape: {placement_logits.shape}")
    print(f"  placement_logitsç»Ÿè®¡:")
    print(f"    min: {placement_logits.min().item():.4f}")
    print(f"    max: {placement_logits.max().item():.4f}")
    print(f"    åŒ…å«inf: {torch.isinf(placement_logits).any()}")
    print(f"    åŒ…å«nan: {torch.isnan(placement_logits).any()}")
    
    # è®¡ç®—æŸå¤±
    batch_loss, batch_accuracy = calculate_placement_loss(
        placement_logits, 
        placement_targets, 
        valid_lengths
    )
    
    print(f"\nğŸ’° æŸå¤±è®¡ç®—:")
    print(f"  batch_loss: {batch_loss}")
    print(f"  batch_accuracy: {batch_accuracy}")
    print(f"  lossæ˜¯å¦ä¸ºinf: {torch.isinf(batch_loss)}")
    print(f"  lossæ˜¯å¦ä¸ºnan: {torch.isnan(batch_loss)}")
    print(f"  lossæ˜¯å¦æœ‰é™: {torch.isfinite(batch_loss)}")
    
    # åå‘ä¼ æ’­
    if torch.isfinite(batch_loss) and batch_loss.item() < 100:
        print(f"\nâ¬…ï¸ æ‰§è¡Œåå‘ä¼ æ’­...")
        optimizer.zero_grad()
        batch_loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        total_grad_norm = 0
        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.data.norm(2)
                total_grad_norm += grad_norm.item() ** 2
        total_grad_norm = total_grad_norm ** 0.5
        
        print(f"  æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        print(f"  âœ… è®­ç»ƒæ­¥éª¤å®Œæˆ")
    else:
        print(f"  âŒ è·³è¿‡å¼‚å¸¸batch: loss={batch_loss}, finite={torch.isfinite(batch_loss)}")

def test_multiple_batches():
    """æµ‹è¯•å¤šä¸ªbatchçš„å¤„ç†"""
    print("\n" + "=" * 80)
    print("ğŸ”„ æµ‹è¯•å¤šä¸ªbatch")
    print("=" * 80)
    
    # åˆ›å»ºå¤šä¸ªä¸åŒçš„batch
    batches = []
    
    for i in range(3):
        sample1 = {
            'bin': torch.tensor([10.0, 8.0]),
            'parts': torch.tensor([[2.0+i*0.1, 3.0], [1.5, 2.0+i*0.1]]),
            'placement_order': torch.tensor([0, 1]),
            'efficiency': 0.85,
            'valid_length': 2
        }
        
        sample2 = {
            'bin': torch.tensor([10.0, 8.0]),
            'parts': torch.tensor([[1.0, 1.0+i*0.1], [2.0, 2.0], [1.5+i*0.1, 1.5]]),
            'placement_order': torch.tensor([1, 0, 2]),
            'efficiency': 0.92,
            'valid_length': 3
        }
        
        batch = [sample1, sample2]
        collated_batch = PlacementOnly_collate_fn(batch)
        batches.append(collated_batch)
    
    # åˆ›å»ºæ¨¡å‹
    model = PlacementOnlyNetwork(input_dim=2, hidden_dim=32, seq_len=10)
    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
    model.train()
    
    total_loss = 0
    total_accuracy = 0
    total_samples = 0
    
    print(f"å¤„ç† {len(batches)} ä¸ªbatch...")
    
    for batch_idx, batch in enumerate(batches):
        print(f"\nBatch {batch_idx + 1}:")
        
        # å‰å‘ä¼ æ’­
        placement_logits = model(
            parts=batch['parts'], 
            bin_info=batch['bins'],
            target_sequence=batch['placement_orders'],
            valid_lengths=batch['valid_lengths']
        )
        
        # è®¡ç®—æŸå¤±
        batch_loss, batch_accuracy = calculate_placement_loss(
            placement_logits, 
            batch['placement_orders'], 
            batch['valid_lengths']
        )
        
        print(f"  loss: {batch_loss:.4f}, accuracy: {batch_accuracy:.3f}")
        
        if torch.isfinite(batch_loss) and batch_loss.item() < 100:
            optimizer.zero_grad()
            batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += batch_loss.item()
            total_accuracy += batch_accuracy
            total_samples += 1
            print(f"  âœ… è®­ç»ƒæˆåŠŸ")
        else:
            print(f"  âŒ è·³è¿‡å¼‚å¸¸batch")
    
    if total_samples > 0:
        avg_loss = total_loss / total_samples
        avg_accuracy = total_accuracy / total_samples
        print(f"\nğŸ“Š æ€»ç»“:")
        print(f"  å¤„ç†çš„batchæ•°: {total_samples}/{len(batches)}")
        print(f"  å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.3f}")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•batch")

if __name__ == "__main__":
    test_training_step()
    test_multiple_batches() 